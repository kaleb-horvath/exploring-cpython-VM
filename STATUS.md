### current status
trying to get a functional tokenizer working. currently at the step of matching each token to ensure validity before
returning the token stream.
